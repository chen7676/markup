# 手写数字探索

## KNN分类

```python
import numpy as np
from sklearn.model_selection import GridSearchCV
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score


def sort_by_target(mnist):
    reorder_train = np.array(sorted([(target, i) for i, target in enumerate(mnist.target[:60000])]))[:, 1]
    reorder_test = np.array(sorted([(target, i) for i, target in enumerate(mnist.target[60000:])]))[:, 1]
    mnist.data[:60000] = mnist.data[reorder_train]
    mnist.target[:60000] = mnist.target[reorder_train]
    mnist.data[60000:] = mnist.data[reorder_test + 60000]
    mnist.target[60000:] = mnist.target[reorder_test + 60000]


try:
    from sklearn.datasets import fetch_openml

    mnist = fetch_openml('mnist_784', version=1, cache=True)
    mnist.target = mnist.target.astype(np.int8)  # fetch_openml() returns targets as strings
    sort_by_target(mnist)  # fetch_openml() returns an unsorted dataset
except ImportError:
    from sklearn.datasets import fetch_mldata

    mnist = fetch_mldata('MNIST original')

X, y = mnist["data"], mnist["target"]

X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]

param_grid = [{'weights': ["uniform", "distance"], 'n_neighbors': [3, 4, 5]}]

knn_clf = KNeighborsClassifier()
grid_search = GridSearchCV(knn_clf, param_grid, cv=5, verbose=3, n_jobs=-1)
grid_search.fit(X_train, y_train)

print(grid_search.best_params_, grid_search.best_score_)

y_pred = grid_search.predict(X_test)
res = accuracy_score(y_test, y_pred)
print(res)

# PCA降维，提高效率，降噪，可能提高了准确率
pca = PCA(0.9)
pca.fit(X_train)
X_train_reduction = pca.transform(X_train)
print(X_train_reduction.shape)
X_test_reduction = pca.transform(X_test)

knn_clf = KNeighborsClassifier()
knn_clf.fit(X_train_reduction, y_train)
knn_score = knn_clf.score(X_test_reduction, y_test)
print(knn_score)
```

## 贝叶斯

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_digits
from sklearn.manifold import Isomap
from sklearn.model_selection import train_test_split, LeaveOneOut
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, confusion_matrix

# 1.加载并可视化手写数字
digits = load_digits()
print(digits.images.shape)  # (1797, 8, 8)
# 图像数据是一个三维矩阵：共有1797个样本，每张图像是8像素*8像素

# 前100张图可视化
# fig, axes = plt.subplots(10, 10, figsize=(8, 8), subplot_kw={'xticks': [], 'yticks': []},
#                          gridspec_kw=dict(hspace=0.1, wspace=0.1))
# for i, ax in enumerate(axes.flat):
#     ax.imshow(digits.images[i], cmap='binary', interpolation='nearest')
#     ax.text(0.05, 0.05, str(digits.target[i]), transform=ax.transAxes, color='green')
# plt.show()

# 特征值和目标值
X = digits.data
y = digits.target
print(X.shape)  # (1797, 64)
print(y.shape)  # (1797,)

# 2.无监督学习：数据降维
iso = Isomap(n_components=2)
iso.fit(X)
data_projected = iso.transform(X)
print(data_projected.shape)  # (1797, 2)

# 数据投影后画图
# plt.scatter(data_projected[:, 0], data_projected[:, 1], c=digits.target, edgecolors='none', alpha=0.5,
#             cmap=plt.cm.get_cmap('Spectral', 10))
# plt.colorbar(label='digit label', ticks=range(10))
# plt.clim(-0.5, 9.5)
# plt.show()

# 3.数字分类
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, random_state=0)

model = GaussianNB()
model.fit(Xtrain, ytrain)
y_model = model.predict(Xtest)

# 模型准确度评价
ac = accuracy_score(ytest, y_model)
print(ac)
# 使用混淆矩阵显示误判率
mat = confusion_matrix(ytest, y_model)
sns.heatmap(mat, square=True, annot=True, cbar=False)
plt.xlabel('predicted value')
plt.ylabel('true value')
# 显示样本和预测标签
fig, axes = plt.subplots(10, 10, figsize=(8, 8),
                         subplot_kw={'xticks': [], 'yticks': []},
                         gridspec_kw = dict(hspace=0.1, wspace=0.1))
test_images = Xtest.reshape(-1, 8, 8)
for i, ax in enumerate(axes.flat):
    ax.imshow(test_images[i], cmap='binary', interpolation='nearest')
    ax.text(0.05, 0.05, str(y_model[i]), transform=ax.transAxes,
            color='green' if (ytest[i] == y_model[i]) else 'red')

plt.show()

```

## 随机森林

```python
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix

digits = load_digits()
print(digits.keys())

# 查看图像
fig = plt.figure(figsize=(6, 6))  # inch
fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)

# 画数字，每个数字是8px*8px
# for i in range(64):
#     ax = fig.add_subplot(8, 8, i + 1, xticks=[], yticks=[])
#     ax.imshow(digits.images[i], cmap=plt.cm.binary, interpolation='nearest')
#     # 用targt做标注
#     ax.text(0, 7, str(digits.target[i]))
# plt.show()

# 用随机森林对数字进行分类
Xtrain, Xtest, ytrain, ytest = train_test_split(digits.data, digits.target, random_state=0)
model = RandomForestClassifier(n_estimators=1000)
model.fit(Xtrain, ytrain)
ypred = model.predict(Xtest)

# 分类结果报告
res = classification_report(ypred, ytest)
print(res)
"""
              precision    recall  f1-score   support

           0       1.00      0.97      0.99        38
           1       1.00      0.96      0.98        45
           2       0.95      1.00      0.98        42
           3       0.98      0.98      0.98        45
           4       0.97      1.00      0.99        37
           5       0.98      0.98      0.98        48
           6       1.00      1.00      1.00        52
           7       1.00      0.96      0.98        50
           8       0.94      0.98      0.96        46
           9       0.98      0.98      0.98        47

    accuracy                           0.98       450
   macro avg       0.98      0.98      0.98       450
weighted avg       0.98      0.98      0.98       450
"""
# 画出混淆矩阵
mat = confusion_matrix(ytest, ypred)
sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False)
plt.xlabel('true label')
plt.ylabel('predicted label')
plt.show()
```

## Isomap

```python
import numpy as np
from matplotlib import offsetbox
import matplotlib.pyplot as plt
from matplotlib.image import imread
import seaborn as sns
from sklearn.datasets import fetch_openml
from sklearn.decomposition import PCA
from sklearn.manifold import MDS, LocallyLinearEmbedding, Isomap, TSNE

mnist = fetch_openml('mnist_784')
print(mnist.data.shape)  # (70000, 784)
print(mnist.target)  # ['5' '0' '4' ... '4' '5' '6']


# 查看图像
# fig, ax = plt.subplots(6, 8, subplot_kw=dict(xticks=[], yticks=[]))
# for i, axi in enumerate(ax.flat):
#     axi.imshow(mnist.data[1250*i].reshape(28, 28), cmap='gray_r')
# plt.show()

# 计算流形学习投影，由于速度影响，仅用数据的1/30进行学习
# data = mnist.data[::30]
# target = mnist.target[::30]
#
# model = Isomap(n_components=2)
# proj = model.fit_transform(data)
# plt.scatter(proj[:, 0], proj[:, 1], cmap=plt.cm.get_cmap('jet', 10))
# plt.colorbar(ticks=range(10))
# plt.clim(-0.5, 9.5)
# plt.show()


# 在不同的投影位置输出图像的缩略图
def plot_components(data, model, images=None, ax=None, thumb_frac=0.05, cmap='gray'):
    ax = ax or plt.gca()
    proj = model.fit_transform(data)
    ax.plot(proj[:, 0], proj[:, 1], '.k')

    if images is not None:
        min_dist_2 = (thumb_frac * max(proj.max(0) - proj.min(0))) ** 2
        shown_images = np.array([2 * proj.max(0)])
        for i in range(data.shape[0]):
            dist = np.sum((proj[i] - shown_images) ** 2, 1)
            if np.min(dist) < min_dist_2:
                # 不展示相距很近的点
                continue
            shown_images = np.vstack([shown_images, proj[i]])
            imagebox = offsetbox.AnnotationBbox(offsetbox.OffsetImage(images[i], cmap=cmap), proj[i])
            ax.add_artist(imagebox)


# 数据比较拥挤，一次只查看一个数字
data = mnist.data[mnist.target == '1'][::4]
fig, ax = plt.subplots(figsize=(10, 10))
model = Isomap(n_neighbors=5, n_components=2, eigen_solver='dense')
plot_components(data, model, images=data.reshape((-1, 28, 28)), ax=ax, thumb_frac=0.05, cmap='gray_r')
plt.show()
# 可以发现数据的异常点，帮助理解数据，提供一些分析数据的线索
```

## Kmeans

```python
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.image import imread
import seaborn as sns
from scipy.stats import mode
from sklearn.datasets import load_digits
from sklearn.cluster import KMeans
from sklearn.manifold import TSNE
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix

digits = load_digits()
print(digits.data.shape)
# (1797, 64)

# kmeans
kmeans = KMeans(n_clusters=10, random_state=0)
clusters = kmeans.fit_predict(digits.data)
print(kmeans.cluster_centers_.shape)
# (10, 64)
# 在64维中有10个类。这些簇中心点本身就是64维像素的点，可以将这些点看成该簇中具有代表性的数字
# fig, ax = plt.subplots(2, 5, figsize=(8, 3))
# centers = kmeans.cluster_centers_.reshape(10, 8, 8)
# for axi, center in zip(ax.flat, centers):
#     axi.set(xticks=[], yticks=[])
#     axi.imshow(center, interpolation='nearest', cmap=plt.cm.binary)

# 即使无标签，也可以找到识别的数字中心，但是1和8是例外(不易识别)
# 由于真是标签未知，因此0～9并不是顺序排列，可以匹配后排序
labels = np.zeros_like(clusters)
print(labels, digits.target)
for i in range(10):
    mask = (clusters == i)
    labels[mask] = mode(digits.target[mask])[0]

# 检查准确性
res = accuracy_score(digits.target, labels)
print(res)  # 0.795
# 混淆矩阵
# mat = confusion_matrix(digits.target, labels)
# sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,
#             xticklabels=digits.target_names, yticklabels=digits.target_names)
# plt.xlabel('true label')
# plt.ylabel('predicted label')
# plt.show()
# 1和8出错最多

# 采用TSNE进行数据预处理
# 投影数据
tsne = TSNE(n_components=2, init='pca', random_state=0)
digits_proj = tsne.fit_transform(digits.data)
# 计算类
kmeans = KMeans(n_clusters=10, random_state=0)
clusters = kmeans.fit_predict(digits_proj)
# 排列标签
labels = np.zeros_like(clusters)
for i in range(10):
    mask = (clusters == i)
    labels[mask] = mode(digits.target[mask])[0]
# 计算准确度
res = accuracy_score(digits.target, labels)
print(res)
# 0.94

```

## GMM

生成新的数据步骤

```
1.首先获得手写数字的示例数据
2.构建该数据的分布模型
3.依据分布模型生成一批新的示例数字
```

示例

```python
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_digits
from sklearn.decomposition import PCA
from sklearn.mixture import GaussianMixture

digits = load_digits()
print(digits.data.shape)  # (1797, 64)


# 画出前100个数据
def plot_digits(data):
    fig, ax = plt.subplots(10, 10, figsize=(8, 8), subplot_kw=dict(xticks=[], yticks=[]))
    fig.subplots_adjust(hspace=0.05, wspace=0.05)
    for i, axi in enumerate(ax.flat):
        im = axi.imshow(data[i].reshape(8, 8), cmap='binary')
        im.set_clim(0, 16)


# plot_digits(digits.data)
# plt.show()

# 现在约有1800个64维度的数字，可以创建一个GMM模型生成更多数字。
# 由于高维度可能不太容易收敛，故先使用一个不可逆的降维算法PCA
pca = PCA(0.99, whiten=True)
data = pca.fit_transform(digits.data)
print(data.shape)  # (1797, 41)

# 对投影数据使用功能AIC，得到GMM成分数量
# n_components = np.arange(50, 210, 10)
# models = [GaussianMixture(n_components=n, covariance_type='full', random_state=0) for n in n_components]
# aics = [model.fit(data).aic(data) for model in models]
# plt.plot(n_components, aics)
# plt.show()

# 使用110个成分并确定收敛性
gmm = GaussianMixture(n_components=110, covariance_type='full', random_state=0)
gmm.fit(data)
print(gmm.converged_)  # True

# 在41维投影空间中画出100个点的示例，将GMM作为生成模型
data_new = gmm.sample(100)
print(data_new[0].shape)  # (100, 41)

# 通过PCA对象逆变换来构建新的数字
digits_new = pca.inverse_transform(data_new[0])
plot_digits(digits_new)
plt.show()

# 这些手写数字不会单独出现在原始数据集中，而是获取u混合模型输入数据的一般特征。这个数字生成模型同时也证明，生成模型是贝叶斯生成分类器的一个非常有用的成分。
```

## Boost

```python
import numpy as np

try:
    from sklearn.datasets import fetch_openml

    mnist = fetch_openml('mnist_784', version=1)
    mnist.target = mnist.target.astype(np.int64)
except ImportError:
    from sklearn.datasets import fetch_mldata

    mnist = fetch_mldata('MNIST original')
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
from sklearn.svm import LinearSVC
from sklearn.neural_network import MLPClassifier
from sklearn.ensemble import VotingClassifier

# 将数据集分为训练集、验证集、测试集
X_train_val, X_test, y_train_val, y_test = train_test_split(
    mnist.data, mnist.target, test_size=10000, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(
    X_train_val, y_train_val, test_size=10000, random_state=42)

# 训练多个分类器
random_forest_clf = RandomForestClassifier(n_estimators=10, random_state=42)
extra_trees_clf = ExtraTreesClassifier(n_estimators=10, random_state=42)
svm_clf = LinearSVC(random_state=42)
mlp_clf = MLPClassifier(random_state=42)

estimators = [random_forest_clf, extra_trees_clf, svm_clf, mlp_clf]
for estimator in estimators:
    print("Training the", estimator)
    estimator.fit(X_train, y_train)

res = [estimator.score(X_val, y_val) for estimator in estimators]
print(res)
"""
[0.9469, 0.9492, 0.8695, 0.9614]
虽然LinearSVC效果较差，但是可作为验证数据验证集成算法
"""

named_estimators = [
    ("random_forest_clf", random_forest_clf),
    ("extra_trees_clf", extra_trees_clf),
    ("svm_clf", svm_clf),
    ("mlp_clf", mlp_clf),
]

voting_clf = VotingClassifier(named_estimators)
voting_clf.fit(X_train, y_train)
res = voting_clf.score(X_val, y_val)
print(res)
res = [estimator.score(X_val, y_val) for estimator in voting_clf.estimators_]
print(res)
"""
0.9622
[0.9469, 0.9492, 0.8695, 0.9614]
"""
# 删除某个训练器
voting_clf.set_params(svm_clf=None)
voting_clf.estimators  # 训练器已更新
voting_clf.estimators_  # 训练过的训练器未更新
# 删除后重新训练，或直接删除训练过的训练器
del voting_clf.estimators_[2]
# 重新评估
res = voting_clf.score(X_val, y_val)
print(res)
"""
0.9638
"""
# soft voting设置，不必重新训练，直接设置即可
voting_clf.voting = "soft"
res = voting_clf.score(X_val, y_val)
print(res)
"""
0.9692
"""
# 对测试集进行测试
res = voting_clf.score(X_test, y_test)
print(res)
res = [estimator.score(X_test, y_test) for estimator in voting_clf.estimators_]
print(res)
"""
0.9697
[0.9437, 0.9474, 0.9608]
"""

```



