# 多元逻辑回归

逻辑回归模型经过推广，可以直接支持多个类别，而不需要训练并组合多个二元分类器。这就是softmax回归，也叫多元逻辑回归。

## 原理

对于一个给定的实例，Softmax回归模型首先计算出每个类别k的分数，然后对这些分数应用softmax函数（也叫归一化指数），估算出每个类别的概率。

对于一个给定的实例 $x$

每个类别k的分数
$$
s_k(x) = \theta_k^T\cdot x
$$
计算完分数后，使用softmax函数计算分数
$$
\hat{p_k}=\sigma(s_k(x)) = \frac{e^{s_k(x)}}{\sum_{j=1}^Ke^{s_j(x)}}
$$
其中，k是类别的数量，$s_k(x)$ 是实例x每个类别的分数的向量，$\hat{p_k}$ 是给定的类别分数下，实例x属于类别k的概率

softmax回归分类器将估算概率值最高的类别作为预测类别
$$
\hat{y} = \mathop{argmax}\limits_{k}{\sigma(s(x))_k} = \mathop{argmax}\limits_{k}{s_k(x)} = \mathop{argmax}\limits_{k}{(\theta_k^T\cdot x)}
$$
交叉熵损失
$$
J(\theta) = - \frac{1}{m}\sum_{i=1}^m\sum_{k=1}^K{y_k^{(i)}\log\hat{p}_k^{(i)}}
$$
对于类别k的交叉梯度向量
$$
\nabla_{\theta_k}J(\theta) = \frac{1}{m}\sum_{i=1}^m{(\hat{p}_k^{(i)}-y_k^{(i)})x^{(i)}}
$$

## sklearn

```python
import os
import numpy as np
from sklearn import datasets
import matplotlib as mpl
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression

mpl.rc('axes', labelsize=14)
mpl.rc('xtick', labelsize=12)
mpl.rc('ytick', labelsize=12)

# Where to save the figures
PROJECT_ROOT_DIR = "."
CHAPTER_ID = "training_linear_models"


def save_fig(fig_id, tight_layout=True):
    path = os.path.join(PROJECT_ROOT_DIR, "images", CHAPTER_ID, fig_id + ".png")
    print("Saving figure", fig_id)
    if tight_layout:
        plt.tight_layout()
    plt.savefig(path, format='png', dpi=300)


# Ignore useless warnings (see SciPy issue #5998)
import warnings

warnings.filterwarnings(action="ignore", message="^internal gelsd")

iris = datasets.load_iris()
list(iris.keys())

X = iris["data"][:, (2, 3)]  # petal length, petal width
y = iris["target"]

softmax_reg = LogisticRegression(multi_class="multinomial", solver="lbfgs", C=10, random_state=42)
softmax_reg.fit(X, y)

x0, x1 = np.meshgrid(
    np.linspace(0, 8, 500).reshape(-1, 1),
    np.linspace(0, 3.5, 200).reshape(-1, 1),
)
X_new = np.c_[x0.ravel(), x1.ravel()]

y_proba = softmax_reg.predict_proba(X_new)
y_predict = softmax_reg.predict(X_new)

zz1 = y_proba[:, 1].reshape(x0.shape)
zz = y_predict.reshape(x0.shape)

plt.figure(figsize=(10, 4))
plt.plot(X[y == 2, 0], X[y == 2, 1], "g^", label="Iris-Virginica")
plt.plot(X[y == 1, 0], X[y == 1, 1], "bs", label="Iris-Versicolor")
plt.plot(X[y == 0, 0], X[y == 0, 1], "yo", label="Iris-Setosa")

from matplotlib.colors import ListedColormap

custom_cmap = ListedColormap(['#fafab0', '#9898ff', '#a0faa0'])

# 显示边界
plt.contourf(x0, x1, zz, cmap=custom_cmap)
contour = plt.contour(x0, x1, zz1, cmap=plt.cm.brg)
plt.clabel(contour, inline=1, fontsize=12)
plt.xlabel("Petal length", fontsize=14)
plt.ylabel("Petal width", fontsize=14)
plt.legend(loc="center left", fontsize=14)
plt.axis([0, 7, 0, 3.5])
save_fig("softmax_regression_contour_plot")
plt.show()

res = softmax_reg.predict([[5, 2]])
print(res)
res = softmax_reg.predict_proba([[5, 2]])
print(res)

```



