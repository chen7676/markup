

# 支持向量机

> Support Vector Machine

SVM尝试寻找一个最优的决策边界，距离两个类别的最近的样本距离最远，即最大化margin=2d。

是一种**判别分类方法**，不再像贝叶斯那样为每类数据建模，而是用一条分割线（二维空间中的直线曲线）或者流形体（多维空间中的曲线曲面等的概念推广）将各种类型分割开。

可以用于线性和非线性分类，也可用于线性和非线性回归，也可用于异常检测。

- 分类

按照线性和非线性：线性SVM、非线性SVM

按照数据分割严格：硬间隔SVM(Hard Margin SVM)、软间隔SVM(Soft Margin SVM)

硬间隔适用于：1.在数据是线性可分离时有效，2.对异常值非常敏感

软间隔适用于：尽可能保持间隔宽阔和限制间隔违例之间平衡

- 优缺点

优点
1.适合小数量样本数据，
2.只受边界线附近的点的影响，可以解决高维问题
3.与核函数配合极具通用性，适用于不同类型的数据

缺点
1.一旦数据量上去了，那么计算机的内存什么的资源就支持不了，这时候LR等算法就比SVM要好。（借助二次规划求解支持向量）
2.训练效果非常依赖边界软化参数C的选择是否合理，需要通过交叉检验自行搜索，数据集大时，计算量比较大
3.预测结果不能直接进行概率解释，可以通过能够过内部交叉检验进行评估(probability参数)，但是评估过程计算量较大

注意：SVM对特征的缩放非常敏感，所以要对数据进行归一化处理。

## 原理

### 硬间隔

$(x, y)$到$Ax + By +C = 0$的距离
$$
\frac{|Ax+By+C|}{\sqrt{A^2+B^2}}
$$
拓展到n维，点到$\theta^Tx_b = 0$直线，其中直线可表示为
$$
w^Tx + b = 0
$$

则距离为
$$
\frac{|w^Tx + b|}{||w||}
$$
其中
$$
||w|| = \sqrt{w_1^2+w_2^2+\ldots+w_n^2}
$$
可得到
$$
\begin{cases}
 \frac{|w^Tx^{(i)} + b|}{||w||}\geq{d} & \forall{y^{(i)}}=1 \\
 \frac{|w^Tx^{(i)} + b|}{||w||}\leq{-d} & \forall{y^{(i)}}=-1
 \end{cases}
$$
变形
$$
\begin{cases}
 \frac{|w^Tx^{(i)} + b|}{||w||d}\geq{1} & \forall{y^{(i)}}=1 \\
 \frac{|w^Tx^{(i)} + b|}{||w||d}\leq{-1} & \forall{y^{(i)}}=-1
 \end{cases}
$$
转换
$$
\begin{cases}
 {w_d^Tx^{(i)} + b_d}\geq{1} & \forall{y^{(i)}}=1 \\
 {w_d^Tx^{(i)} + b_d}\leq{-1} & \forall{y^{(i)}}=-1
 \end{cases}
$$
则三条直线分别为
$$
w_d^Tx + b_d = 1 \\
w_d^Tx + b_d = 0 \\
w_d^Tx + b_d = -1
$$
为了便于书写
$$
w^Tx + b = 1 \\
w^Tx + b = 0 \\
w^Tx + b = -1
$$

$$
\begin{cases}
 {w^Tx^{(i)} + b}\geq{1} & \forall{y^{(i)}}=1 \\
 {w^Tx^{(i)} + b}\leq{-1} & \forall{y^{(i)}}=-1
 \end{cases}
$$

可转化为
$$
y^{(i)}(w^Tx^{(i)} + b)\geq{1}
$$
对于任意支撑向量x
$$
max\frac{|w^Tx + b|}{||w||} \\
max\frac{1}{||w||} \\
min{||w||}  \\
min\frac{1}{2}||w||^2
$$
可以得到有条件的最优化问题
$$
min\frac{1}{2}||w||^2 \\
st.y^{(i)}(w^Tx^{(i)} + b)\geq{1}
$$

求解

```
拉格朗日乘子法
```

### 软间隔

C越小，容错空间越大，C越大，容错空间越小

L1正则
$$
min\frac{1}{2}||w||^2 + C\sum_{i=1}^m\zeta_i\\
st.y^{(i)}(w^Tx^{(i)} + b)\geq{1}-\zeta_i \\
\zeta_i\geq{0}
$$
 L2正则
$$
min\frac{1}{2}||w||^2 + C\sum_{i=1}^m\zeta_i^2\\
st.y^{(i)}(w^Tx^{(i)} + b)\geq{1}-\zeta_i \\
\zeta_i\geq{0}
$$
转换为(拉格朗日乘子法+求偏导)
$$
max\sum_{i=1}^m\alpha_i - \frac{1}{2}\sum_{i=1}^m\sum_{i=1}^m\alpha_i\alpha_jy_iy_jx_ix_j\\
st. 0\leq\alpha_i\leq{C} \\
\sum_{i=1}^m\alpha_iy_i = 0
$$
使用核函数技巧$K(x, y)=x'\cdot{y}'$
$$
max\sum_{i=1}^m\alpha_i - \frac{1}{2}\sum_{i=1}^m\sum_{i=1}^m\alpha_i\alpha_jy_iy_jK(x_i, x_j)\\
st. 0\leq\alpha_i\leq{C} \\
\sum_{i=1}^m\alpha_iy_i = 0
$$

### 核技巧

在实际应用中，很多是线性不可分的数据，核技巧的作用是通过将线性不可分的输入特征向量映射到高维空间中，使得映射后的结果在高维空间能够通过超平面分离。

但是点乘数据计算时间复杂度过高，在计算中发现高维数据点乘数据等价于低维数据点乘的平方，故计算时间复杂度降低，不需真实映射到高维空间，只需将高维数据点乘直接替换为低微数据点乘的平方即可，这就是核技巧的本质。

核函数(Kernel Function)就是一种输入两个低纬空间向量、返回高维空间点积的函数。使用SVM训练数据即可以选择一些通用的核函数，也可以自定义核函数。

一些常用的核函数如下

- 线性核
$$
K(x, y) = x\cdot{y}\\
$$
直接返回输入向量的点积，速度最快。因为实际上没有升维，适用于本身特征纬度较高、样本数量很大的场景。

- 多项式核

二次多项式核函数
$$
K(x, y) = (x\cdot{y}+1)^2 \\
K(x, y) = (\sum_{i=1}^n{x_iy_j + 1})^2
$$
扩展多元多项式核函数
$$
K(x, y) = (x\cdot{y}+c)^d \\
$$

- 高斯径向基核（RBF核）

$$
k(p, q) = e^{-\gamma||p-q||^2}
$$

$\gamma$参数值越大越容易过拟合，是使用最广法的SVM核。

- sigmoid核

$$
k(p, q) = tanh(a\times p\cdot q + r)，其中tanh(x) = \frac{e^x-e^{-x}}{e^x+ e^{-x}}
$$

有两个超参数$a,r$可以调整，也是一种非线性核。



核函数选用

```
1.优先尝试LinearSVC，特别是训练集非常大或特征非常多时
2.训练集不大时，试用高斯BRF，大多数情况ok
3.时间和计算充裕时，采用交叉验证和网格搜索尝试其他核函数

常用核函数：linear、poly、rbf
```

## sklearn

### svm展示

```python
import matplotlib.pyplot as plt
import numpy as np
from sklearn.datasets import make_blobs, make_circles
from sklearn.svm import SVC


# 辅助函数画出SVM的决策边界
def plot_svc_decision_function(model, ax=None, plot_support=True):
    """画二维SVC的决策函数"""
    if ax is None:
        ax = plt.gca()
    xlim = ax.get_xlim()
    ylim = ax.get_ylim()
    # 创建评估模型的网格
    x = np.linspace(xlim[0], xlim[1], 30)
    y = np.linspace(ylim[0], ylim[1], 30)
    Y, X = np.meshgrid(y, x)
    xy = np.vstack([X.ravel(), Y.ravel()]).T
    P = model.decision_function(xy).reshape(X.shape)
    # 画出决策边界和边界
    ax.contour(X, Y, P, colors='k', levels=[-1, 0, 1], alpha=0.5, linestyles=['--', '-', '--'])

    # 画支持向量
    if plot_support:
        ax.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1],
                   s=300, linewidth=1, facecolors='none')
    ax.set_xlim(xlim)
    ax.set_ylim(ylim)


# 1.拟合支持向量机
# 模拟数据
X, y = make_blobs(n_samples=50, centers=2, random_state=0, cluster_std=0.60)
# # plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')
#
# 支持向量机的由来
# xfit = np.linspace(-1, 3.5)
# plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')
# plt.plot([0.6], [2.1], 'x', color='red', markeredgewidth=2, markersize=10)
#
# # for m, b in [(1, 0.65), (0.5, 1.6), (-0.2, 2.9)]:
# #     plt.plot(xfit, m * xfit + b, '-k')
#
# for m, b, d in [(1, 0.65, 0.33), (0.5, 1.6, 0.55), (-0.2, 2.9, 0.2)]:
#     yfit = m * xfit + b
#     plt.plot(xfit, yfit, '-k')
#     plt.fill_between(xfit, yfit - d, yfit + d, edgecolor='none', color='#AAAAAA', alpha=0.4)
#
# plt.xlim(-1, 3.5)

# model = SVC(kernel='linear', C=1E10)
# model.fit(X, y)
#
# plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')
# plot_svc_decision_function(model)
#
# print(model.support_vectors_)  # 支持向量坐标点
"""
[[0.44359863 3.11530945]
 [2.33812285 3.43116792]
 [2.06156753 1.96918596]]
"""

# 2.核函数SVM模型
X, y = make_circles(100, factor=0.1, noise=0.1)

# clf = SVC(kernel='linear').fit(X, y)
# plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')
# plot_svc_decision_function(clf, plot_support=False)
# plt.show()

# clf = SVC(kernel='rbf', C=1E6).fit(X, y)
# plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')
# plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],
#             s=300, lw=1, facecolors='none')
# plot_svc_decision_function(clf)

# 3.软化边界
X, y = make_blobs(n_samples=100, centers=2, random_state=0, cluster_std=1.2)

# plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')

fig, ax = plt.subplots(1, 2, figsize=(16, 6))
fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)
# 超参数C的不同边界变化
for axi, C in zip(ax, [10.0, 0.1]):
    model = SVC(kernel='linear', C=C).fit(X, y)
    axi.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')
    plot_svc_decision_function(model, axi)
    axi.scatter(model.support_vectors_[:, 0],
                model.support_vectors_[:, 1],
                s=300, lw=1, facecolors='none')
    axi.set_title('C = {0:.1f}'.format(C), size=14)

plt.show()

```

### 线性SVM

实现

```python
# 方法一
LinearSVC(C=1, loss="hinge", random_state=42)  # 会自动对偏置项进行正则化，故需提前减去平均值，使训练集集中。
# 方法二
SVC(kernel="linear", C=1)  # 速度较慢，特别是大型训练集不推荐使用，使用LinearSVC替代
# 方法三
SGDClassifier(loss="hinge", alpha=1/(m*C))  # 适用于常规随机梯度下降，不会像LinearSVC那样快速瘦脸，但是对于内存处理不了的大型数据集(核外训练)或在线分类任务，非常有效
```

示例

```python
import numpy as np
from sklearn import datasets
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.svm import LinearSVC

iris = datasets.load_iris()
X = iris["data"][:, (2, 3)]  # petal length, petal width
y = (iris["target"] == 2).astype(np.float64)  # Iris-Virginica

svm_clf = Pipeline([
        ("scaler", StandardScaler()),
        ("linear_svc", LinearSVC(C=1, loss="hinge", random_state=42)),
    ])

svm_clf.fit(X, y)
res = svm_clf.predict([[5.5, 1.7]])
print(res)
"""
[1.]
"""
```

### 多项式特征

对于不可线性可分离的情况，可以添加更多特征，依靠升维使得原本线性不可分的数据线性可分

添加多项式特征实现简单，且对所有的机器学习算法都非常有效，但是如果多项式太低阶，处理不了非常复杂的数据集，若是太高阶，则会创造出大量的特征，导致模型变得太慢。

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.processing import PolynomialFeatures
from sklearn.processing import StandardScaler
from sklearn.svm import LinearSVC
from sklearn.pipeline import Pipeline

X, y = datasets.make_moons(n_samples=100, random_state=42)

def plot_dataset(X, y, axes):
    plt.plot(X[:, 0][y==0], X[:, 1][y==0], "bs")
    plt.plot(X[:, 0][y==1], X[:, 1][y==1], "g^")
    plt.axis(axes)
    plt.grid(True, which='both')
    plt.xlabel(r"$x_1$", fontsize=20)
    plt.ylabel(r"$x_2$", fontsize=20, rotation=0)
    
    
plt_dataset(X, y, [-1.5, 2.5, -1, 1.5])
plt.show()

X, y = datasets.make_moons(n_samples=100, noise=0.15, random_state=42)

plt_dataset(X, y, [-1.5, 2.5, -1, 1.5])
plt.show()

# 使用多项式特征的SVM
def PolynomialSVC(degree, C=1.0):
  	return Pipeline([
      	("poly", PolynomialFeatures(degree=degree)),
      	("std_scaler", StandardScaler()),
      	("linearSVC", LinearSVC(C=C, loss="hinge", random_state=42))
    ])
  
poly_svc = PolynomialSVC(degree=3)
poly_svc.fit(X, y)

# 决策边界
def plot_decision_boundary(model, axis):
  	x0, x1 = np.meshgrid(
    	np.linspace(axis[0], axis[1], int((axis[1]-axis[0])*100)).reshape(-1, 1),
      	np.linspace(axis[2], axis[3], int((axis[3]-axis[2])*100)).reshape(-1, 1)
    )
    X_new = np.c_[x0.ravel(), x1.ravel()]
    y_predict = model.predict(X_new)
    zz = y_predict.reshape(x0.shape)
    from matplotlib.colors import ListedColormap
    custom_cmap = ListedColormap(["#EF9A9A", "#FF59D", "#90CAF9"])
    
    plt.contourf(x0, x1, zz, linewidth=5, cmap=custom_cmap)
    
plot_decision_boundary(poly_svc, axis=[-1.5, 2.5, -1.0, 1.5])

plt_dataset(X, y, [-1.5, 2.5, -1, 1.5])
plt.show() 
```

### 相似特征

解决非线性问题的另一种技术是添加相似特征。这些特征经过相似函数计算得出，相似函数可以测量每个实例与一个特定地标(landmark)之间的相似度。例如一维数据集为例，在$x_1=-2,x_1=1$ 处添加两个地标，使用高斯径向基函数(RBF)作为相似函数。
$$
\phi\gamma(x, l) = e^{-\gamma||x-l||^2}
$$
令 $\gamma=0.3$，这是一个从0（离地标非常远）到1（跟地标一样）变化的钟形函数。计算新的特征，看实例$x_1=-1$，其距离第一个地标距离为1，距离第二个地标的距离为2，则新的特征为$x_2=e^{-0.3\times 1^2}=0.74, x_3=e^{-0.3\times 2^2}=0.30$ ，转换后的数据集数据呈线性可分离的了。

选择地标的方法，最简单的是在数据集里每一个实例的位置上创建一个地标，这会创建出许多维度，因而增加了转换后的训练集线性可分离的机会。缺点是，一个有m个实例n个特征的训练集会转换成一个m个实例m个特征的训练集（假设抛弃了原始特征）。如果训练集非常大，就会的到同样大数量的特征。

```python
import os
import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt


mpl.rc('axes', labelsize=14)
mpl.rc('xtick', labelsize=12)
mpl.rc('ytick', labelsize=12)

# Where to save the figures
PROJECT_ROOT_DIR = "."
CHAPTER_ID = "training"

def save_fig(fig_id, tight_layout=True):
    path = os.path.join(PROJECT_ROOT_DIR, "images", CHAPTER_ID, fig_id + ".png")
    print("Saving figure", fig_id)
    if tight_layout:
        plt.tight_layout()
    plt.savefig(path, format='png', dpi=300)


def gaussian_rbf(x, landmark, gamma):
    return np.exp(-gamma * np.linalg.norm(x - landmark, axis=1) ** 2)


# 展示转换前后
gamma = 0.3
X1D = np.linspace(-4, 4, 9).reshape(-1, 1)
x1s = np.linspace(-4.5, 4.5, 200).reshape(-1, 1)
x2s = gaussian_rbf(x1s, -2, gamma)
x3s = gaussian_rbf(x1s, 1, gamma)

XK = np.c_[gaussian_rbf(X1D, -2, gamma), gaussian_rbf(X1D, 1, gamma)]
yk = np.array([0, 0, 1, 1, 1, 1, 1, 0, 0])

plt.figure(figsize=(11, 4))

plt.subplot(121)
plt.grid(True, which='both')
plt.axhline(y=0, color='k')
plt.scatter(x=[-2, 1], y=[0, 0], s=150, alpha=0.5, c="red")
plt.plot(X1D[:, 0][yk == 0], np.zeros(4), "bs")
plt.plot(X1D[:, 0][yk == 1], np.zeros(5), "g^")
plt.plot(x1s, x2s, "g--")
plt.plot(x1s, x3s, "b:")
plt.gca().get_yaxis().set_ticks([0, 0.25, 0.5, 0.75, 1])
plt.xlabel(r"$x_1$", fontsize=20)
plt.ylabel(r"Similarity", fontsize=14)
plt.annotate(r'$\mathbf{x}$',
             xy=(X1D[3, 0], 0),
             xytext=(-0.5, 0.20),
             ha="center",
             arrowprops=dict(facecolor='black', shrink=0.1),
             fontsize=18,
             )
plt.text(-2, 0.9, "$x_2$", ha="center", fontsize=20)
plt.text(1, 0.9, "$x_3$", ha="center", fontsize=20)
plt.axis([-4.5, 4.5, -0.1, 1.1])

plt.subplot(122)
plt.grid(True, which='both')
plt.axhline(y=0, color='k')
plt.axvline(x=0, color='k')
plt.plot(XK[:, 0][yk == 0], XK[:, 1][yk == 0], "bs")
plt.plot(XK[:, 0][yk == 1], XK[:, 1][yk == 1], "g^")
plt.xlabel(r"$x_2$", fontsize=20)
plt.ylabel(r"$x_3$  ", fontsize=20, rotation=0)
plt.annotate(r'$\phi\left(\mathbf{x}\right)$',
             xy=(XK[3, 0], XK[3, 1]),
             xytext=(0.65, 0.50),
             ha="center",
             arrowprops=dict(facecolor='black', shrink=0.1),
             fontsize=18,
             )
plt.plot([-0.1, 1.1], [0.57, -0.1], "r--", linewidth=3)
plt.axis([-0.1, 1.1, -0.1, 1.1])

plt.subplots_adjust(right=1)

save_fig("kernel_method_plot")
plt.show()

# 计算相似特征转换
x1_example = X1D[3, 0]
for landmark in (-2, 1):
    k = gaussian_rbf(np.array([[x1_example]]), np.array([[landmark]]), gamma)
    print("Phi({}, {}) = {}".format(x1_example, landmark, k))
"""
Phi(-1.0, -2) = [0.74081822]
Phi(-1.0, 1) = [0.30119421]
"""
```

### 核函数

#### 多项式

添加多项式特征后，可能会增加计算时间，使用核技巧，它产生的结果就跟添加了许多多项式特征一样，但实际上并不需要真的添加。因为实际没有添加任何特征，所以也就不存在数量爆炸的组合特征。

```python
from sklearn.svm import SVC

def PolynomialKernelSVC(degree, C=1.0):
  	return Pipeline([
      	("std_scaler", StandardScaler()),
      	("kenelSVC", SVC(kenel="poly", degree, C=C))
    ])
  
poly_kernel_svc = PolynomialKernelSVC(degree=3)
poly_kernel_svc.fit(X, y)

# 决策边界
plot_decision_boundary(poly_svc, axis=[-1.5, 2.5, -1.0, 1.5])
plt.scatter(X_standard[y==0, 0], X_standard[y==0, 1])
plt.scatter(X_standard[y==1, 0], X_standard[y==1, 1])
plt.show() 
```

#### 高斯

与多项式特征一样，相似特征法也可以用于任意机器学习算法，但是要计算出所有附加特征，计算代价昂贵。核技巧继续发挥作用，能够产生的结果与添加了许多相似特征一样，但实际上也并不需要添加。

```python
import os
import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.datasets import make_moons

mpl.rc('axes', labelsize=14)
mpl.rc('xtick', labelsize=12)
mpl.rc('ytick', labelsize=12)

# Where to save the figures
PROJECT_ROOT_DIR = "."
CHAPTER_ID = "training"


def save_fig(fig_id, tight_layout=True):
    path = os.path.join(PROJECT_ROOT_DIR, "images", CHAPTER_ID, fig_id + ".png")
    print("Saving figure", fig_id)
    if tight_layout:
        plt.tight_layout()
    plt.savefig(path, format='png', dpi=300)


X, y = make_moons(n_samples=100, noise=0.15, random_state=42)

# 绘制数据
def plot_dataset(X, y, axes):
    plt.plot(X[:, 0][y == 0], X[:, 1][y == 0], "bs")
    plt.plot(X[:, 0][y == 1], X[:, 1][y == 1], "g^")
    plt.axis(axes)
    plt.grid(True, which='both')
    plt.xlabel(r"$x_1$", fontsize=20)
    plt.ylabel(r"$x_2$", fontsize=20, rotation=0)

# 绘制边界
def plot_predictions(clf, axes):
    x0s = np.linspace(axes[0], axes[1], 100)
    x1s = np.linspace(axes[2], axes[3], 100)
    x0, x1 = np.meshgrid(x0s, x1s)
    X = np.c_[x0.ravel(), x1.ravel()]
    y_pred = clf.predict(X).reshape(x0.shape)
    y_decision = clf.decision_function(X).reshape(x0.shape)
    plt.contourf(x0, x1, y_pred, cmap=plt.cm.brg, alpha=0.2)
    plt.contourf(x0, x1, y_decision, cmap=plt.cm.brg, alpha=0.1)


# 不通的超参数对应的分类情况
gamma1, gamma2 = 0.1, 5
C1, C2 = 0.001, 1000
hyperparams = (gamma1, C1), (gamma1, C2), (gamma2, C1), (gamma2, C2)

svm_clfs = []
for gamma, C in hyperparams:
    rbf_kernel_svm_clf = Pipeline([
        ("scaler", StandardScaler()),
        ("svm_clf", SVC(kernel="rbf", gamma=gamma, C=C))
    ])
    rbf_kernel_svm_clf.fit(X, y)
    svm_clfs.append(rbf_kernel_svm_clf)

plt.figure(figsize=(11, 7))

for i, svm_clf in enumerate(svm_clfs):
    plt.subplot(221 + i)
    plot_predictions(svm_clf, [-1.5, 2.5, -1, 1.5])
    plot_dataset(X, y, [-1.5, 2.5, -1, 1.5])
    gamma, C = hyperparams[i]
    plt.title(r"$\gamma = {}, C = {}$".format(gamma, C), fontsize=16)

save_fig("moons_rbf_svc_plot")
plt.show()
"""
增加gamma值会使钟形曲线变窄，因此每个实例的影响范围随之变小，决策边界变得更加不规则，开始围着单个实例绕弯；
减小gamma值会使钟形曲线变宽，因而每个实例的影响范围随之增大，决策边界变得更平坦，故gamma类似一个正则化参数，模型过度拟合，则降低其值，若拟合不足，则增加其值
"""
```

### 计算复杂度

`liblinear`库为线性SVM实现了一个优化算法，`LinearSVC`正是基于该库的，此算法不支持核技巧，不过它与训练实例的数量和特征数量几乎呈线性相关：其训练时间复杂度大致为$O(m\times n)$。

若是想要非常高的精度，算法需要的时间更长，它由容差超参数$\epsilon$ 控制（sklearn中为`tol`），大多数分类任务中，默认的容差就够了。

SVC则是基于`libsvm`库的，该库算法支持核技巧。训练时间复杂度通常在 $O(m^2\times n)~O(m^3\times n)$之间。这意味着如果训练实例的数量变大（如十万以上），它将变得很慢，所以完美适用于复杂但是中小型训练集。但是，它还是可以良好适应地特征数量的增加，特别是应对稀疏特征。在此情况下，算法复杂度大致与实例的平均非零特征数成比例。

| 类              | 时间复杂度                      | 是否支持核外 | 是否需要缩放 | 核技巧 |
| --------------- | ------------------------------- | ------------ | ------------ | ------ |
| `LinearSVC`     | $O(m\times n)$                  | 否           | 是           | 否     |
| `SGDClassifier` | $O(m\times n)$                  | 是           | 是           | 否     |
| `SVC`           | $O(m^2\times n)~O(m^3\times n)$ | 否           | 是           | 是     |

### 回归问题

SVM不仅支持线性和非线性分类，还支持线性和非线性回归。

原理是：不再是尝试拟合两个类别之间可能的最宽的间隔同时限制间隔违例（间隔中的实例），SVM回归是让尽可能多的实例位于间隔中，同时限制间隔违例（不在间隔中的实例）。间隔的宽度由超参数 $\epsilon$ 控制。

在间隔内添加更多的实例不会影响模型的预测，所以这个模型被称为 $\epsilon$ 不敏感。

```python
# 线性回归
svm_reg = LinearSVR(epsilon=1.5, random_state=42)
# 非线性回归
svm_poly_reg = SVR(kernel="poly", degree=2, C=100, epsilon=0.1, gamma="auto")
```

线性回归

```python
import os
import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt
from sklearn.svm import LinearSVR

mpl.rc('axes', labelsize=14)
mpl.rc('xtick', labelsize=12)
mpl.rc('ytick', labelsize=12)

# Where to save the figures
PROJECT_ROOT_DIR = "."
CHAPTER_ID = "training"


def save_fig(fig_id, tight_layout=True):
    path = os.path.join(PROJECT_ROOT_DIR, "images", CHAPTER_ID, fig_id + ".png")
    print("Saving figure", fig_id)
    if tight_layout:
        plt.tight_layout()
    plt.savefig(path, format='png', dpi=300)


np.random.seed(42)
m = 50
X = 2 * np.random.rand(m, 1)
y = (4 + 3 * X + np.random.randn(m, 1)).ravel()



# 不同的epsilon值，控制不同的间隔，epsilon越大，间隔越大
svm_reg1 = LinearSVR(epsilon=1.5, random_state=42)
svm_reg2 = LinearSVR(epsilon=0.5, random_state=42)
svm_reg1.fit(X, y)
svm_reg2.fit(X, y)


def find_support_vectors(svm_reg, X, y):
    y_pred = svm_reg.predict(X)
    off_margin = (np.abs(y - y_pred) >= svm_reg.epsilon)
    return np.argwhere(off_margin)


svm_reg1.support_ = find_support_vectors(svm_reg1, X, y)
svm_reg2.support_ = find_support_vectors(svm_reg2, X, y)

eps_x1 = 1
eps_y_pred = svm_reg1.predict([[eps_x1]])


def plot_svm_regression(svm_reg, X, y, axes):
    x1s = np.linspace(axes[0], axes[1], 100).reshape(100, 1)
    y_pred = svm_reg.predict(x1s)
    plt.plot(x1s, y_pred, "k-", linewidth=2, label=r"$\hat{y}$")
    plt.plot(x1s, y_pred + svm_reg.epsilon, "k--")
    plt.plot(x1s, y_pred - svm_reg.epsilon, "k--")
    plt.scatter(X[svm_reg.support_], y[svm_reg.support_], s=180, facecolors='#FFAAAA')
    plt.plot(X, y, "bo")
    plt.xlabel(r"$x_1$", fontsize=18)
    plt.legend(loc="upper left", fontsize=18)
    plt.axis(axes)


plt.figure(figsize=(9, 4))
plt.subplot(121)
plot_svm_regression(svm_reg1, X, y, [0, 2, 3, 11])
plt.title(r"$\epsilon = {}$".format(svm_reg1.epsilon), fontsize=18)
plt.ylabel(r"$y$", fontsize=18, rotation=0)
# plt.plot([eps_x1, eps_x1], [eps_y_pred, eps_y_pred - svm_reg1.epsilon], "k-", linewidth=2)
plt.annotate(
    '', xy=(eps_x1, eps_y_pred), xycoords='data',
    xytext=(eps_x1, eps_y_pred - svm_reg1.epsilon),
    textcoords='data', arrowprops={'arrowstyle': '<->', 'linewidth': 1.5}
)
plt.text(0.91, 5.6, r"$\epsilon$", fontsize=20)
plt.subplot(122)
plot_svm_regression(svm_reg2, X, y, [0, 2, 3, 11])
plt.title(r"$\epsilon = {}$".format(svm_reg2.epsilon), fontsize=18)
save_fig("svm_regression_plot")
plt.show()
```

非线性回归

```python
import os
import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt
from sklearn.svm import SVR

mpl.rc('axes', labelsize=14)
mpl.rc('xtick', labelsize=12)
mpl.rc('ytick', labelsize=12)

# Where to save the figures
PROJECT_ROOT_DIR = "."
CHAPTER_ID = "training"


def save_fig(fig_id, tight_layout=True):
    path = os.path.join(PROJECT_ROOT_DIR, "images", CHAPTER_ID, fig_id + ".png")
    print("Saving figure", fig_id)
    if tight_layout:
        plt.tight_layout()
    plt.savefig(path, format='png', dpi=300)


np.random.seed(42)
m = 100
X = 2 * np.random.rand(m, 1) - 1
y = (0.2 + 0.1 * X + 0.5 * X ** 2 + np.random.randn(m, 1) / 10).ravel()

# 不同的C值，不同的正则化程度，C越小，正则化越大
# 使用二阶多项式核
svm_poly_reg1 = SVR(kernel="poly", degree=2, C=100, epsilon=0.1, gamma="auto")
svm_poly_reg2 = SVR(kernel="poly", degree=2, C=0.01, epsilon=0.1, gamma="auto")
svm_poly_reg1.fit(X, y)
svm_poly_reg2.fit(X, y)


def plot_svm_regression(svm_reg, X, y, axes):
    x1s = np.linspace(axes[0], axes[1], 100).reshape(100, 1)
    y_pred = svm_reg.predict(x1s)
    plt.plot(x1s, y_pred, "k-", linewidth=2, label=r"$\hat{y}$")
    plt.plot(x1s, y_pred + svm_reg.epsilon, "k--")
    plt.plot(x1s, y_pred - svm_reg.epsilon, "k--")
    plt.scatter(X[svm_reg.support_], y[svm_reg.support_], s=180, facecolors='#FFAAAA')
    plt.plot(X, y, "bo")
    plt.xlabel(r"$x_1$", fontsize=18)
    plt.legend(loc="upper left", fontsize=18)
    plt.axis(axes)


plt.figure(figsize=(9, 4))
plt.subplot(121)
plot_svm_regression(svm_poly_reg1, X, y, [-1, 1, 0, 1])
plt.title(r"$degree={}, C={}, \epsilon = {}$".format(svm_poly_reg1.degree, svm_poly_reg1.C, svm_poly_reg1.epsilon),
          fontsize=18)
plt.ylabel(r"$y$", fontsize=18, rotation=0)
plt.subplot(122)
plot_svm_regression(svm_poly_reg2, X, y, [-1, 1, 0, 1])
plt.title(r"$degree={}, C={}, \epsilon = {}$".format(svm_poly_reg2.degree, svm_poly_reg2.C, svm_poly_reg2.epsilon),
          fontsize=18)
save_fig("svm_with_polynomial_kernel_plot")
plt.show()

```

### SVM训练

分类

```python
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.svm import LinearSVC, SVC
from sklearn.metrics import accuracy_score
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import reciprocal, uniform

try:
    from sklearn.datasets import fetch_openml

    mnist = fetch_openml('mnist_784', version=1, cache=True)
except ImportError:
    from sklearn.datasets import fetch_mldata

    mnist = fetch_mldata('MNIST original')

X = mnist["data"]
y = mnist["target"]

X_train = X[:60000]
y_train = y[:60000]
X_test = X[60000:]
y_test = y[60000:]

# 许多训练算法对顺序敏感，故打乱顺序
np.random.seed(42)
rnd_idx = np.random.permutation(60000)
X_train = X_train[rnd_idx]
y_train = y_train[rnd_idx]

# 以简单线性分类为起始，自动OVR
lin_clf = LinearSVC(random_state=42)
lin_clf.fit(X_train, y_train)

# 使用准确度评估
y_pred = lin_clf.predict(X_train)
res = accuracy_score(y_train, y_pred)
print(res)
"""
86%, 准确率过低，肯能是模型复杂度过低，也可能数据问题
"""
# 标准化
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train.astype(np.float32))
X_test_scaled = scaler.transform(X_test.astype(np.float32))

lin_clf = LinearSVC(random_state=42)
lin_clf.fit(X_train_scaled, y_train)

y_pred = lin_clf.predict(X_train_scaled)
res = accuracy_score(y_train, y_pred)
print(res)
"""
准确度提高，但是仍然较低，使用带kernel的SVM
"""
# 在版本<0.19，SVC默认OVO，需要制定OVR
svm_clf = SVC(decision_function_shape="ovr", gamma="auto")
svm_clf.fit(X_train_scaled[:10000], y_train[:10000])

y_pred = svm_clf.predict(X_train_scaled)
res = accuracy_score(y_train, y_pred)
print(res)
"""
即使数据集较少，训练效果明显
"""
# 使用交叉验证随机搜索，确定超参数
param_distributions = {"gamma": reciprocal(0.001, 0.1), "C": uniform(1, 10)}
rnd_search_cv = RandomizedSearchCV(svm_clf, param_distributions, n_iter=10, verbose=2, cv=3)
rnd_search_cv.fit(X_train_scaled[:1000], y_train[:1000])
print(rnd_search_cv.best_estimator_, rnd_search_cv.best_score_)
# 训练所有数据集
rnd_best_estimator = rnd_search_cv.best_estimator_
rnd_best_estimator.fit(X_train_scaled, y_train)
y_pred = rnd_best_estimator.predict(X_train_scaled)
res = accuracy_score(y_train, y_pred)
print(res)
# 测试集测试
y_pred = rnd_best_estimator.predict(X_test_scaled)
res = accuracy_score(y_test, y_pred)
print(res)

```

回归

```python
import numpy as np
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import LinearSVR
from sklearn.metrics import mean_squared_error
from sklearn.svm import SVR
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import reciprocal, uniform

# 获取数据
housing = fetch_california_housing()
X = housing["data"]
y = housing["target"]

# 拆分训练集、测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 标准化
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 先使用LinearSVR
lin_svr = LinearSVR(random_state=42)
lin_svr.fit(X_train_scaled, y_train)

# 评估
y_pred = lin_svr.predict(X_train_scaled)
mse = mean_squared_error(y_train, y_pred)
res = np.sqrt(mse)
print(res)

# 使用检查验证随机搜索，确定超参数
param_distributions = {"gamma": reciprocal(0.001, 0.1), "C": uniform(1, 10)}
rnd_search_cv = RandomizedSearchCV(SVR(), param_distributions, n_iter=10, verbose=2, cv=3, random_state=42)
rnd_search_cv.fit(X_train_scaled, y_train)

rnd_search_cv_best = rnd_search_cv.best_estimator_

y_pred = rnd_search_cv.best_estimator_.predict(X_train_scaled)
mse = mean_squared_error(y_train, y_pred)
res = np.sqrt(mse)
print(res)
# 在测试集上测试
y_pred = rnd_search_cv.best_estimator_.predict(X_test_scaled)
mse = mean_squared_error(y_test, y_pred)
res = np.sqrt(mse)
print(res)

```

### 

