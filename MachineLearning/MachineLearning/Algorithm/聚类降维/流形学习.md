# 流形学习

PCA虽然是一个灵活、快速且容易解释的算法，但是它对存在**非线性**关系的数据集的处理效果并不太好。

流形学习弥补了这个缺陷。流形学习是一种无监督评估器，试图将一个低维度流形嵌入到一个高纬度空间来描述数据集。

- 常见sklearn-API

LLE

```python
from sklearn.manifold import LocallyLinearEmbedding
# 对于简单问题，如S曲线、局部线性嵌入及其变体学习效果非常好
```

Isomap

```python
from sklearn.manifold import Isomap
# 对现实世界的高纬度数据源学习效果较好
```

t-SNE

```python
from sklearn.manifold import TSNE
# 用于高度聚类的数据效果较好，但是学习速度较慢
```

- 与PCA对比

```
1.在流形学习中，并没有好的框架来处理缺失值。相比之下，PCA算法中有一个用于处理缺失值的迭代方法
2.在流形学习中，数据中噪音的出现将造成流形短路，并且严重影响嵌入结果。相比之下，PCA可以自然地从最重要的成分中滤除噪音
3.流形嵌入的结果通常高度依赖所选取的邻节点的个数，并且通常没有确定的定量方式来选择最优的邻节点个数。相比之下，PCA算法中并不存在这样的问题
4.在流形学习中，全局最优的输出维度数很难确定。相比之下，PCA可以基于解释方差来确定输出的维度数
5.在流形学习中，嵌入维度的含义并不总是很清楚。在PCA算法中，主成分有非常明确的含义
6.在流形学习中，流形方法的计算复杂度为O[N^2]或O[N^3]。而PCA可以选择随机方法，通常速度更快
```

虽然以上为流形学习相比PCA算法的缺点，但是流形学习还有一个明显的优点：它具有保留数据中的非线性关系的能力。通常做法：先用PCA探索数据的线性特征，再用流形方法探索数据的非线性特征。

## 线性嵌入

线性嵌入：将数据旋转、平移和缩放到一个高维空间的操作。

多维标度法（MDS）
虽然从(x,y)坐标计算这个距离矩阵很简单，但是从距离矩阵转换回x坐标和y坐标值却比较困难
多维标度法可以解决这个问题：可以将一个数据集的距离矩阵还原称一个D维坐标来表示数据集

```python
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.image import imread
import seaborn as sns
from sklearn.manifold import MDS
from sklearn.metrics import pairwise_distances
from mpl_toolkits import mplot3d


# 定义一个流形
def make_hello(N=1000, rseed=42):
    #  画出"HELLO"文字形状的图像，并保存为PNG
    fig, ax = plt.subplots(figsize=(4, 1))
    fig.subplots_adjust(left=0, right=1, bottom=0, top=1)
    ax.axis('off')
    ax.text(0.5, 0.4, 'HELLO', va='center', ha='center', weight='bold', size=85)
    fig.savefig('hello.png')
    plt.close(fig)

    # 打开png，并将一些随机点画进去
    data = imread('hello.png')[::-1, :, 0].T
    rng = np.random.RandomState(rseed)
    X = rng.rand(4 * N, 2)
    i, j = (X * data.shape).astype(int).T
    mask = (data[i, j] < 1)
    X = X[mask]
    X[:, 0] *= (data.shape[0] / data.shape[1])
    X = X[:N]
    return X[np.argsort(X[:, 0])]


X = make_hello(1000)
colorize = dict(c=X[:, 0], cmap=plt.cm.get_cmap('rainbow', 5))


# plt.scatter(X[:, 0], X[:, 1], **colorize)
# plt.axis('equal')
# plt.show()

# 用旋转矩阵旋转数据，x和y的值会改变，但是数据形状基本还是一样
def rotate(X, angle):
    theta = np.deg2rad(angle)
    R = [[np.cos(theta), np.sin(theta)], [-np.sin(theta), np.cos(theta)]]
    return np.dot(X, R)


X2 = rotate(X, 20) + 5
# plt.scatter(X2[:, 0], X2[:, 1], **colorize)
# plt.axis('equal')
# plt.show()
# 说明x和y的值并不是数据间关系的必要基础特征。真正的基础特征是每个点与数据集中其他点的距离
# 表示这种关系的常用方法是关系（距离）矩阵：对于N个点，构建一个N*N的矩阵，元素(i,j)是点i和点j之间的距离

# 计算关系矩阵
D = pairwise_distances(X)
print(D.shape)  # (1000, 1000)
# N=1000个点，获得一个1000*1000的矩阵
# 画出矩阵
# plt.imshow(D, zorder=2, cmap='Blues', interpolation='nearest')
# plt.colorbar()
# plt.show()
# 为做过旋转和平移的数据构建一个距离矩阵
D2 = pairwise_distances(X2)
print(np.allclose(D, D2))  # True
# 这个距离矩阵给出了一个数据集内部关系的表现形式，这种形式与数据集的旋转和投影无关。
# 距离矩阵的可视化效果不够直观

# 多维标度法（MDS）
# 虽然从(x,y)坐标计算这个距离矩阵很简单，但是从距离矩阵转换回x坐标和y坐标值却比较困难
# 多维标度法可以解决这个问题：可以将一个数据集的距离矩阵还原称一个D维坐标来表示数据集
# model = MDS(n_components=2, dissimilarity='precomputed', random_state=1)
# out = model.fit_transform(D)
# plt.scatter(out[:, 0], out[:, 1], **colorize)
# plt.axis('equal')
# plt.show()
# 仅仅依靠描述数据点间关系的N*N距离矩阵 ，MDS算法就可以为数据还原出一种可行二维坐标

# 将MSD用于流形学习
# 三维空间
def random_project(X, dimension=3, rseed=42):
    assert dimension >= X.shape[1]
    rng = np.random.RandomState(rseed)
    C = rng.randn(dimension, dimension)
    e, v = np.linalg.eigh(np.dot(C, C.T))
    return np.dot(X, v[:X.shape[1]])


X3 = random_project(X, 3)
print(X3.shape)  # (1000, 3)

# 画出图
# ax = plt.axes(projection='3d')
# ax.scatter3D(X3[:, 0], X3[:, 1], X3[:, 2], **colorize)
# ax.view_init(azim=70, elev=50)
# plt.show()

# 计算三维数据的距离矩阵，得出距离矩阵的最优二维嵌入结果
model = MDS(n_components=2, random_state=1)
out3 = model.fit_transform(X3)
# plt.scatter(out3[:, 0], out3[:, 1], **colorize)
# plt.axis('equal')
# plt.show()

```
## 非线形嵌入

当嵌入是非线性时，即超越简单的操作集合时，MDS算法失效。

```python
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.image import imread
import seaborn as sns
from mpl_toolkits import mplot3d
from sklearn.manifold import MDS


# 定义一个流形
def make_hello(N=1000, rseed=42):
    #  画出"HELLO"文字形状的图像，并保存为PNG
    fig, ax = plt.subplots(figsize=(4, 1))
    fig.subplots_adjust(left=0, right=1, bottom=0, top=1)
    ax.axis('off')
    ax.text(0.5, 0.4, 'HELLO', va='center', ha='center', weight='bold', size=85)
    fig.savefig('hello.png')
    plt.close(fig)

    # 打开png，并将一些随机点画进去
    data = imread('hello.png')[::-1, :, 0].T
    rng = np.random.RandomState(rseed)
    X = rng.rand(4 * N, 2)
    i, j = (X * data.shape).astype(int).T
    mask = (data[i, j] < 1)
    X = X[mask]
    X[:, 0] *= (data.shape[0] / data.shape[1])
    X = X[:N]
    return X[np.argsort(X[:, 0])]


X = make_hello(1000)
colorize = dict(c=X[:, 0], cmap=plt.cm.get_cmap('rainbow', 5))


# plt.scatter(X[:, 0], X[:, 1], **colorize)
# plt.axis('equal')
# plt.show()

# 定义一个三维空间中扭曲成S的形状
def make_hello_s_curve(X):
    t = (X[:, 0] - 2) * 0.75 * np.pi
    x = np.sin(t)
    y = X[:, 1]
    z = np.sign(t) * (np.cos(t) - 1)
    return np.vstack((x, y, z)).T


XS = make_hello_s_curve(X)

# 画图
# ax = plt.axes(projection='3d')
# ax.scatter3D(XS[:, 0], XS[:, 1], XS[:, 2], **colorize)
# plt.show()
# 虽然数据点间基本的关系仍然存在，但是这次数据以非线性的方式进行了变换：被包裹成了S形
# MSD
model = MDS(n_components=2, random_state=2)
outS = model.fit_transform(XS)
plt.scatter(outS[:, 0], outS[:, 1], **colorize)
plt.axis('equal')
plt.show()
# 尝试使用MDS处理这个数据，则无法展示数据非线性嵌入的特征，进而导致丢失了这个嵌入式流形的内部基本关系特性
```

## 局部线性嵌入

MSD生成的嵌入模型，会试图保留数据集中每对数据点间的距离。局部性嵌入(LLE)生成的嵌入模型，该方法不保留所有的距离，仅保留邻节点的距离。

LLE算法通过某种方式将卷曲的数据展开，并且线段的长度基本保持不变。通过对成本函数的全局优化来反映这个逻辑。

```python
# 局部线性嵌入
from sklearn.manifold import LocallyLinearEmbedding
model = LocallyLinearEmbedding(n_neighbors=100, n_components=2, method='modified', eigen_solver='dense')
out = model.fit_transform(XS)
fig, ax = plt.subplots()
ax.scatter(out[:, 0], out[:, 1], **colorize)
ax.set_ylim(0.15, -0.15)
plt.show()
# 虽然有一定变形，但是保留了数据的基本关系特性
```

