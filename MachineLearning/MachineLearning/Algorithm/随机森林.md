# 随机森林

随机森林是一个包含多个决策树的分类器，并且其输出的类别是由个别树输出的类别的众数而定。

是一种集成方法，通过集成多个比较简单的评估器形成累积效果。

- 优缺点

优点

1. 能够解决单个决策树不稳定的情况

2. 能够处理具有高维特征的输入样本，而且不需要降维（使用的是特征子集）
3. 对于缺省值问题也能够获得很好得结果（投票）

4. 在训练完成后，能够给出哪些feature比较重要

5. 容易做成并行化方法，速度比较快

6. 可以进行可视化， 便于分析

缺点

随机森林已经被证明在某些噪音较大的分类或回归问题上会过拟合


## 原理

Bagging + Base Estimator: Decision Tree

决策树在节点划分上，在随机的特征子集上寻找最优划分特征。

注意：理论上越多的树效果越好，但实际上基本超过一定数量就上下浮动了

- Extra-Tree

随机森林里单颗树的生长过程中，每个节点在分裂时仅考虑到一个随机子集所包含的特征。如果对每个特征使用随机阈值，而不是搜索得出的最佳阈值，则可能让决策树生长得更加随机。

这种极端随机的巨册书组成的森林，被称为**极端随机树**集成（Exta-Tree）。

决策树在节点划分上，使用随机的特征和随机的阈值

提供了额外的随机性，抑制过拟合，以更高的偏差换取了更低的方差。

比常规的决策树有更快的训练速度，因为在每个节点上找到每个特征的最佳阈值时决策树生长中最耗时的任务之一。

- 特征重要性

查看单个决策树，重要的特征更可能出现在靠近根节点的位置，而不重要的特征通常出现在靠近叶节点的位置(甚至根本不出现)。因此，通过计算一个特征在森林中的所有树上的平均深度，可以估算出一个特征的重要程度。

## sklearn

### 随机森林

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_moons
from sklearn.ensemble import RandomForestClassifier, BaggingClassifier
from sklearn.tree import DecisionTreeClassifier

X, y = make_moons(n_samples=500, noise=0.3, random_state=666)

plt.scatter(X[y == 0, 0], X[y == 0, 1])
plt.scatter(X[y == 1, 0], X[y == 1, 1])
plt.show()

# 方法一：bagging
bag = BaggingClassifier(
    DecisionTreeClassifier(splitter="random"),
    n_estimators=500,
    bootstrap=True,
    oob_score=True,
    n_jobs=-1,
    random_state=42
)
bag.fit(X, y)
print(bag.oob_score_)

# 方法二：RandomForestClassifier
rf_clf = RandomForestClassifier(
    n_estimators=500,  # 森林中树木的数量
    bootstrap=True,  # 可放回的抽样
    oob_score=True,
    n_jobs=-1,
    random_state=42,
)
rf_clf.fit(X, y)
print(rf_clf.oob_score_)

# 修改参数
rf_clf2 = RandomForestClassifier(
    n_estimators=500,
    bootstrap=True, 
    max_leaf_nodes=16,
    oob_score=True,
    n_jobs=-1,
    random_state=42,
)
rf_clf2.fit(X, y)
print(rf_clf2.oob_score_)
```

### Extra-Tree

```python
from sklearn.ensemble import ExtraTreesClassifier

et_clf = ExtraTreesClassifier(
	n_estimators=500,
  	bootstrap=True,
  	oob_score=True,
  	random_state=42,
)
et_clf.fit(X, y)
print(et_clf.oob_score_)
```

### 特征重要性

通过变量`feature_importances_`在训练结束后自动计算每个特征的重要性。

```python
from sklearn.datasets import load_iris
from sklearn.ensemble import RandomForestClassifier

iris = load_iris()
rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1, random_state=42)
rnd_clf.fit(iris["data"], iris["target"])
for name, score in zip(iris["feature_names"], rnd_clf.feature_importances_):
    print(name, score)

print(rnd_clf.feature_importances_)
"""
sepal length (cm) 0.11249225099876375
sepal width (cm) 0.02311928828251033
petal length (cm) 0.4410304643639577
petal width (cm) 0.4233579963547682
[0.11249225 0.02311929 0.44103046 0.423358  ]
"""
```

### 回归问题

```python
from sklearn.ensemble import BaggingRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import ExtraTreeRegressor
```

示例

```python
import numpy as np
import pandas as pd
import matplotlib as mpl
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from sklearn.ensemble import RandomForestRegressor


# 快慢震荡组合数据
def model(x, sigma=0.3):
    fast_oscillation = np.sin(5 * x)
    slow_oscillation = np.sin(0.5 * x)
    noise = sigma * rng.randn(len(x))
    return slow_oscillation + fast_oscillation + noise


rng = np.random.RandomState(42)
x = 10 * rng.rand(200)

y = model(x)
# plt.errorbar(x, y, 0.3, fmt='o')
# plt.show()

# 使用随机森林回归
forest = RandomForestRegressor(200)
forest.fit(x[:, None], y)

xfit = np.linspace(0, 10, 1000)
yfit = forest.predict(xfit[:, None])
ytrue = model(xfit, sigma=0)

plt.errorbar(x, y, 0.3, fmt='o', alpha=0.5)
plt.plot(xfit, yfit, '-g')
plt.plot(xfit, ytrue, '-k', alpha=0.5)
plt.show()

```

### 随机森林训练

示例1

```python
import numpy as np

import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import RandomForestClassifier


# 对分类器结果进行可视化
def visualize_classifier(model, X, y, ax=None, cmap='rainbow'):
    ax = ax or plt.gca()
    # 画出训练数据
    ax.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=cmap, clim=(y.min(), y.max()), zorder=3)
    ax.axis('tight')
    ax.axis('off')
    xlim = ax.get_xlim()
    ylim = ax.get_ylim()
    # 用评估器拟合数据
    model.fit(X, y)
    xx, yy = np.meshgrid(np.linspace(*xlim, num=200), np.linspace(*ylim, num=200))
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)
    # 为结果生成彩色图
    n_classes = len(np.unique(y))
    contours = ax.contourf(xx, yy, Z, alpha=0.3, levels=np.arange(n_classes + 1) - 0.5,
                           cmap=cmap, zorder=1)
    ax.set(xlim=xlim, ylim=ylim)


# 创建一颗决策树
X, y = make_blobs(n_samples=300, centers=4, random_state=0, cluster_std=1.0)
# plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')
# plt.show()


# 集成袋装分类器
# tree = DecisionTreeClassifier()
# bag = BaggingClassifier(tree, n_estimators=100, max_samples=0.8, random_state=1)
# visualize_classifier(bag, X, y)
# plt.show()

# 随机森林
model = RandomForestClassifier(n_estimators=100, random_state=0)
visualize_classifier(model, X, y)
plt.show()
```

示例2

```python
# 1.导入合适的包
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier,export_graphviz
from sklearn.metrics import classification_report
from sklearn.ensemble import RandomForestClassifier

# 2.加载数据
data = pd.read_csv("http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.txt")
# 3.数据处理
# 填补缺失值age
data["age"].fillna(data["age"].mean(), inplace=True)
# 雷彪数据进行One-Hot编码
data = pd.get_dummies(data, columns=["pclass", "sex"]) 
print(data.head(2))
# 4.特征选择和数据集分割
# 特征值
x = data[["age", "pclass_1st", "pclass_2nd", "pclass_3rd", "sex_female", "sex_male"]]
# 目标值
y = data["survived"]
# 数据集分割
x_train, x_test, y_train, y_test = train_test_split(x, y)
# 5.随机森林估计器流程
rfc = RandomForestClassifier(n_estimators=5, criterion="entropy", max_depth=4)
rfc.fit(x_train, y_train)
# 5.预测
predict = rfc.predict(x_test)
# 6.准确率
score = rfc.score(x_test, y_test)
print(score)
```

