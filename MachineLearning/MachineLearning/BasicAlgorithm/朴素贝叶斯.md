# 朴素贝叶斯

优缺点

```
# 优点
1.朴素贝叶斯模型发源于古典数学理论，有稳定的分类效率。
2.对缺失数据不太敏感，算法也比较简单，常用于文本分类。
3.分类准确度高，速度快
4.容易解释
5.可调参数非常少

# 缺点
1.需要知道先验概率P(F1,F2,…|C)，因此在某些时候会由于假设的先验模型的原因导致预测效果不佳
```

适用场景

```
1.假设分布函数与数据匹配
2.各种类型的区分度很高，模型复杂度不重要
3.非常高维度的数据，模型复杂度不重要
```

## 原理

贝叶斯定理
$$
P(B|A)=\frac{P(A|B)P(B)}{P(A)}
$$

当事件(特征)相互独立时，贝叶斯准则转变为朴素贝叶斯，朴素贝叶斯是贝叶斯准则中的一种特殊情况。

在贝叶斯分类中，我们希望确定一个具有某些特征的样本属于某类标签的概率，记为 $P(L|特征) $，可用如下公式计算
$$
P(L|特征) = \frac{P(特征｜L)P(L)}{P(特征)}
$$
假如需要确定两种标签，定义为 $L_1,L_2$，一种方法就是计算这两个标签的后验概率的比值
$$
\frac{P(L_1|特征)}{P(L_2|特征)} = \frac{P(特征｜L_1)P(L_1)}{P(特征｜L_2)P(L_2)}
$$
现在需要一种模型，帮我们计算每个标签的 $P(特征|L_i)$。这种模型称为**生成模型**。因为它可以训练出生成数据的假设随机过程（或称为概率分布）。为每个标签生成模型是贝叶斯分类器训练过程的主要部分。虽然这个训练步骤通常很难做，但是可以通过对模型进行随机分布的假设，来简化训练工作。

在给定输入数据x的条件下，它属于类别c的概率为
$$
p(c|x ) = \frac{p(x|c)*p(c)}{p(x)}
$$
贝叶斯的分类思想是，不直接求取$p(c|x)$，而是通过求取$p(x|c),p(c)$，利用条件概率公式来寻找最优分类。在一般情况下，不需要考虑$p(x)$ 的值，因为对于同一个输入数据，$p(x)$ 的值是相同的。

贝叶斯分类的难点在于求取 $p(c|x)$，则要求取在条件c的条件下，输入属性值的联合分布，当输入属性很多时，计算量非常大，为此考虑这样一个假设，即输入数据的向量中，属性值时按类别条件独立，满足
$$
p(x|c) = p(x_1|c)*p(x_2|c)*\cdots p(x_n|c)
$$
要使用朴素贝叶斯模型进行分类，需要确定其参数，也就是求解$p(c_k),p(x_i|c_k)$。参数训练过程如下：

设由m个训练数据构成的数据集D
$$
D = \{(x^1, y^1),(x^2, y^2),\cdots,(x^m, y^m)\}
$$
其中，
$$
x^i = (x_1^i, x_2^i,\cdots,x_n^i)\\
y^i \in \{c_1,c_2,\cdots,c_r\}\\
x_j^i \in \{a_1, a_2,\cdots, a_p\}
$$
$x^i$ 表示第i个样本数据的输入，$y^i$ 表示第i个样本数据的输出，$x_j^i$ 表示第i个样本数据的输入特征向量的第j个特征。

参数的训练策略采用极大似然估计，也就是用训练的频率来估计概率
$$
p(c_k) = \frac{\sum_{i=1}^{m}1_{y^i=c_k}}{m}
$$
其中，$1_{y^i=c_k}$ 是一个指示函数，满足
$$
1_{y^i=c_k}=
\begin{cases}
1 & y^i=c_k\\
0 & y^i\ne c_k
\end{cases}
$$
同理，对于$p(x_i|c_k)$ 有
$$
p(x_j^i=a_t|y^i=c_k) = \frac{\sum_{i=1}^{m}1_{y^i=c_k,x_j^i=a_t}}{\sum_{i=1}^{m}1_{y^i=c_k}}
$$

> 注意

使用朴素贝叶斯网络进行分类时，需要注意零概率问题，由于训练数据是有限的，因此极有可能出现 $p(x_i|c)=0$ 的情形，这时要用到平滑的策略来解决零概率问题。

## 分类

之所以称为**朴素**，是因为如果对每种标签的生成模型进行非常简单的假设，就能找到每种类型生成模型的近似解，然后就可以使用贝叶斯分类。不同类型的朴素贝叶斯分类器是由对数据的不同假设决定的。

- 高斯朴素贝叶斯

假设每个标签的数据都服从简单的高斯分布

- 多项式朴素贝叶斯

假设特征是由一个简单多项式分布生成。

多项分布可以描述各种类型样本出现次数的概率，因此多项式朴素贝叶斯非常适合用于描述出现次数或者出现次数比例的特征。

## sklearn

### API

```python
# 高斯朴素贝叶斯
from sklearn.naive_bayes import GaussianNB

# 多项式朴素贝叶斯
from sklearn.naive_bayes import MultinomialNB
```

- 拉普拉斯平滑

```python
由于样本数较少，会出现p(A|B)的概率为0，防止此情况出现，使用拉普莱斯平滑

拉普拉斯平滑系数ɑ, 默认为1
p=Ni/N    ---> p=(Ni+a)/(N+am)
m为训练文档中特征词个数，Ni为xi在分类ci下出现的次数，N为分类ci下词频总数。

MultinomialNB(alpha=0.1)
```

### 示例

- 高斯朴素贝叶斯

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from sklearn.naive_bayes import GaussianNB

X, y = make_blobs(100, 2, centers=2, random_state=2, cluster_std=1.5)
# plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='RdBu')
# plt.show()
# 训练
model = GaussianNB()
model.fit(X, y)
# 预测新数据
rng = np.random.RandomState(0)
Xnew = [-6, -14] + [14, 18] * rng.rand(2000, 2)
ynew = model.predict(Xnew)

# 画图寻找决策边界的位置
plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='RdBu')
lim = plt.axis()
plt.scatter(Xnew[:, 0], Xnew[:, 1], c=ynew, s=20, cmap='RdBu', alpha=0.1)
plt.axis(lim)
plt.show()

# 计算某个样本属于某个标签的概率
yprob = model.predict_proba(Xnew)
print(yprob[-8:].round(2))
# [[0.89 0.11]
#  [1.   0.  ]
#  [1.   0.  ]
#  [1.   0.  ]
#  [1.   0.  ]
#  [1.   0.  ]
#  [0.   1.  ]
#  [0.15 0.85]]
# 前两个标签的后验概率。若要评估分类器的不确定性，这类贝叶斯方法很有用
```

- 多项式朴素贝叶斯

```python
# 1.导入需要的包
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB

# 2.载入数据
news = fetch_20newsgroups(subset="all")
# 3.特征选取
# 特征值,文章内容
x = news.data
# 目标值，文章的类别
y = news.target
print(len(y))
# 4.分割训练集
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=1)
# 5.TF-IDF生成文章特征词
# 特征抽取
cv = TfidfVectorizer()
x_train = cv.fit_transform(x_train)  # 词频矩阵
x_test = cv.transform(x_test)  # 按照训练集抽取特征词统计词频
# 6.朴素贝叶斯estimator流程进行预估
mnb = MultinomialNB()
mnb.fit(x_train, y_train)
mnb.predict(x_test)
score = mnb.score(x_test, y_test)
print(score)
```

示例2

```python
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import make_pipeline
from sklearn.metrics import confusion_matrix

data = fetch_20newsgroups()
print(data.target_names)
# ['alt.atheism', 'comp.graphics', ..., 'talk.politics.misc', 'talk.religion.misc']
# 选择四类新闻
categories = ['talk.religion.misc', 'soc.religion.christian', 'sci.space', 'comp.graphics']
train = fetch_20newsgroups(subset='train', categories=categories)
test = fetch_20newsgroups(subset='test', categories=categories)
# 创建管道，将TF-IDF向量化同多项式朴素贝叶斯分类器组合
model = make_pipeline(TfidfVectorizer(), MultinomialNB())
# 训练数据
model.fit(train.data, train.target)
labels = model.predict(test.data)

# 评估评估器性能
# 使用混淆矩阵统计测试数据的真是标签与预测标签的结果
mat = confusion_matrix(test.target, labels)
sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,
            xticklabels=train.target_names, yticklabels=train.target_names)
plt.xlabel('true label')
plt.ylabel('predicted label')
plt.show()


# 快速返回字符串的预测结果
def predict_category(s, train=train, model=model):
    pred = model.predict([s])
    return train.target_names[pred[0]]


res = predict_category('sending a payload to the ISS')
print(res)  # sci.space

```

