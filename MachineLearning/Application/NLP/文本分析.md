# 文本分析

文本分析和NLP是现代人工智能系统不可分割的一部分。NLP最常用的领域包括：搜索引擎、情感分析、主题建模、词性标注、实体识别等。

分词、词性标注、去除停用词、分块划分文本、词袋模型、句子情感、主题建模等，见[NLTK](../BasicUtils/NLTK.md)

## 词嵌入

词嵌入(Word Embedding)可以将文本和词语转换为机器能够接受的数值向量

### 原理

- 语言的表示

> 符号主义

```
符号主义中典型的代表是Bag of words，即词袋模型。基于词袋模型可以方便地用一个N维向量表示任何一句话，每个维度的值即对应的词出现的次数。

优点：简单
缺点：当词典中词的数量增大时，向量的维度将随之增大；无论是词还是句子的表示，向量过于稀疏，除了少数维度外其他维度均为0；每个词对应的向量在空间上都两两正交，任意一堆向量之间的内积等数值特征为0，无法表达词语之间的语义关联和差异；句子的向量表示丢失了词序特征，"我很不高兴"和“不我很高兴”对应的向量相同
```

> 分布式表示

```
分布式表示的典型代表是Word Embedding，即词嵌入。使用低维、稠密、实值得词向量来表示每一个词，从而赋予词语丰富的语义含义，并使得计算词语相关度成为可能。两个词具有语义相关或相似，则它们对应得词向量之间距离相近，度量向量之间的距离可以使用经典的欧拉距离和余弦相似度等。

词嵌入可以将词典中的每个词映射成对应的词向量，好的词嵌入模型具有：相关性好，类比关联
```

- 训练词向量

词向量的训练主要是基于无监督学习，从大量文本语料中学习出每个词的最佳词向量。训练的核心思想是，语义相关或相似的词语，大多具有相似的上下文，即它们经常在相似的语境中出现。

词嵌入模型中的典型代表是Word2Vec

### 实现

gensim是开源python工具包，用于从非结构化文本中无监督地学习文本隐层的主题向量表示，支持包括TF-IDF,LSA,LDA和Word2Vec在内的多种主题模型算法，并提供了诸如相似度计算、信息检索等常用任务的API接口。

语料：维基百科，[搜狗新闻](http://www.sogou.com/labs/resource/cs.php)

英文案例

```python
# 加载包
from gensim.models import Word2Vec
from gensim.models.word2vec import LineSentence

# 训练模型
sentences = LineSentence('wiki.zh.word.text')
# size词向量的维度，window上下文环境的窗口大小，min_count忽略出现次数低于min_count的词
model = Word2Vec(sentences, size=128, window=5， min_count=5, workers=4)

# 保存模型
model.save('word_embedding_128')

# 若已经保存过模型，则直接加载即可
# 训练及保存代码可省略
# model = Word2Vec.load('word_embedding_128')

# 使用模型
# 返回一个词语最相关的多个词语及对应的相关度
items = model.most_similar(u'中国')
for item in items:
    # 词的内容，词的相关度
    print item[0], item[1]
# 返回连个词语之间的相关度
model.similarity(u'男人', u'女人')
```

中文案例

```python
import sys
import multiprocessing
from gensim.models import Word2Vec
from gensim.models.word2vec import LineSentence


def save_model():
    """
    保存模型
    :return:
    """
    if len(sys.argv) < 3:
        sys.exit(1)

    # inp表示语料分词，outp表示模型
    inp, outp = sys.argv[1:3]
    """
    Word2Vec(LineSentence(inp), size=400, window=5, min_count=5)
    # 参数
    LineSentence(inp)：把word2vec训练模型的磁盘存储文件，转换成所需要的格式,如：[[“sentence1”],[”sentence1”]]
    size：是每个词的向量维度
    window：是词向量训练时的上下文扫描窗口大小，窗口为5就是考虑前5个词和后5个词
    min-count：设置最低频率，默认是5，如果一个词语在文档中出现的次数小于5，那么就会丢弃
    # 方法：
    inp:分词后的文本
    save(outp1):保存模型
    """
    model = Word2Vec(LineSentence(inp), size=400, window=5, min_count=5, workers=multiprocessing.cpu_count())
    model.save(outp)


def predict_model():
    """
    测试模型
    :return:
    """
    """
    model = gensim.models.Word2Vec.load("*.model")
    model.most_similar('警察')
    model.similarity('男人','女人')
    most_similar(positive=['女人', '丈夫'], negative=['男人'], topn=1)
    """
    model = Word2Vec.load("./model/corpus.model")
    res = model.most_similar("警察")
    print(res)


if __name__ == '__main__':
    # 保存模型
    save_model()
    """
    终端运行python trainword2vec.py ./corpus_seg.txt ./model/*
    """
    # 测试模型
    predict_model()
```

