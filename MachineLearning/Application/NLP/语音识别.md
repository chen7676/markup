# 语音识别

语音识别是指识别和理解口语的过程。输入音频数据，语音识别器将处理这些数据，从中提取出有用的信息。语音识别应用：声音控制设备、将语音转换成单词、安全系统等。

## 读取和绘制音频数据

音频文件是实际音频信号的数字化形式，实际的音频信号是复杂的连续波形。为了将其保存成数字形式，需要对音频信息进行采样并将其转换成数字。例如，语音通常以44100Hz的频率进行采样，这就意味着每秒钟信号被分解成44100份，然后这些抽样值被保存。也就是每隔1/44100s都会存储一次值。如果采样率很高，用媒体播放器收听音频时，会感觉到信号是连续的。

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.io import wavfile

# 读取输入文件
sampling_freq, audio = wavfile.read('input_read.wav')
# 输出音频信号的相关参数
print('Shape:', audio.shape)
print('Datatype:', audio.dtype)
print('Duration:', round(audio.shape[0] / float(sampling_freq), 3), 'seconds')

# 音频信号被存储在一个16位有符号整型数据中
# 标准化数值
audio = audio / (2. ** 15)

# 抽取前30个值画图
audio = audio[:30]

# 建立时间轴
x_values = np.arange(0, len(audio), 1) / float(sampling_freq)

# 将单位转换为秒
x_values *= 1000

# 绘制声音信号图形
plt.plot(x_values, audio, color='black')
plt.xlabel('Time (ms)')
plt.ylabel('Amplitude')
plt.title('Audio signal')
plt.show()

```

## 将音频信号转换为频域

音频信号是不同频率、幅度和相位的正弦波的复杂混合。正弦波也称作正弦曲线。音频信号的频率内容中隐藏了很多信息。事实上，一个音频信号的性质由其频率内容决定。世界上的语音和音乐都是基于这个事实的。傅立叶变换实现了将音频信号转换为频域。

```python
import numpy as np
from scipy.io import wavfile
import matplotlib.pyplot as plt

sampling_freq, audio = wavfile.read('input_freq.wav')

# 标准化信号
audio = audio / (2.**15)

# 获取数组长度
len_audio = len(audio)

# 进行傅立叶变换
# 由于傅立叶变化是关于中心对称的，所以只需要转换信号的前半部分。
# 目的是提取功率信号，因此需现将信号的值平方
transformed_signal = np.fft.fft(audio)
half_length = np.ceil((len_audio + 1) / 2.0)
transformed_signal = abs(transformed_signal[0:half_length])
transformed_signal /= float(len_audio)
transformed_signal **= 2

# 提取信号的长度
len_ts = len(transformed_signal)

# 将部分信号乘以2
if len_audio % 2:
    transformed_signal[1:len_ts] *= 2
else:
    transformed_signal[1:len_ts-1] *= 2

# 获取功率信号
power = 10 * np.log10(transformed_signal)

# 建立时间轴
x_values = np.arange(0, half_length, 1) * (sampling_freq / len_audio) / 1000.0

# 绘图
plt.figure()
plt.plot(x_values, power, color='black')
plt.xlabel('Freq (in kHz)')
plt.ylabel('Power (in dB)')
plt.show()
```

## 生成音频信号

音频信号是一些正弦波的复杂混合，可以基于该原理生成自定义参数的音频信号

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.io.wavfile import write

# 定义存储音频的文件
output_file = 'output_generated.wav'

# 指定音频生成的参数
duration = 3  # seconds
sampling_freq = 44100  # Hz
tone_freq = 587
min_val = -2 * np.pi
max_val = 2 * np.pi

# 生成音频信号
t = np.linspace(min_val, max_val, duration * sampling_freq)
audio = np.sin(2 * np.pi * tone_freq * t)

# 增加噪声
noise = 0.4 * np.random.rand(duration * sampling_freq)
audio += noise

# 将这些值转换为16位整型数进行保存
scaling_factor = pow(2,15) - 1
audio_normalized = audio / np.max(np.abs(audio))
audio_scaled = np.int16(audio_normalized * scaling_factor)

# 写入输出文件
write(output_file, sampling_freq, audio_scaled)

# 提取前100个值
audio = audio[:100]

# 生成时间轴
x_values = np.arange(0, len(audio), 1) / float(sampling_freq)

# 将时间轴的单位转换为秒
x_values *= 1000

# 绘图
plt.plot(x_values, audio, color='black')
plt.xlabel('Time (ms)')
plt.ylabel('Amplitude')
plt.title('Audio signal')
plt.show()
```

## 合成音乐

http://www/phy.mtu.edu/~suits/notefreqs.html列举了各种音阶及其相应的频率，可以使用生成音频的方式合成音乐。

```python
import json
import numpy as np
from scipy.io.wavfile import write
import matplotlib.pyplot as plt

# 定义合成音调
def synthesizer(freq, duration, amp=1.0, sampling_freq=44100):
    # 创建时间轴
    t = np.linspace(0, duration, duration * sampling_freq)

    # 构建音频信号：使用幅度和频率参数
    audio = amp * np.sin(2 * np.pi * freq * t)

    return audio.astype(np.int16) 

if __name__=='__main__':
    # 一些音阶及其频率
    tone_map_data = """
    {
    	"A": 440,
    	"Asharp": 466,
    	"B": 494,
    	"C": 523,
    	"Csharp": 554,
    	"D": 587,
    	"Dsharp": 622,
    	"E": 659,
    	"F": 698,
    	"Fsharp": 740,
    	"G": 784,
    	"Gsharp": 831
	}
    """
    tone_freq_map = json.loads(tone_map_data)
        
    # 设置生成G调的输入参数
    input_tone = 'G'
    duration = 2     # seconds
    amplitude = 10000
    sampling_freq = 44100    # Hz

    # 生成音阶
    synthesized_tone = synthesizer(tone_freq_map[input_tone], duration, amplitude, sampling_freq)

    # 写入输出文件
    write('output_tone.wav', sampling_freq, synthesized_tone)

    # 音阶及其持续时间
    tone_seq = [('D', 0.3), ('G', 0.6), ('C', 0.5), ('A', 0.3), ('Asharp', 0.7)]

    # 构建基于和弦序列的音频信号
    output = np.array([])
    for item in tone_seq:
        input_tone = item[0]
        duration = item[1]
        synthesized_tone = synthesizer(tone_freq_map[input_tone], duration, amplitude, sampling_freq)
        output = np.append(output, synthesized_tone, axis=0)

    # 写入输出文件
    write('output_tone_seq.wav', sampling_freq, output)

```

## 提取频域特征

在多数的现代语音识别系统中，人们都会用到频域的特征。将信号转换为频域之后，还需要将其转换成有用的形式。**梅尔频率倒谱系数**(Mel Frequency Cepstrum. Coefficient, MFCC)可以解决这个问题。MFCC首先计算信号的功率谱，然后用滤波器组和离散余弦变换的组合来提取特征。

```shell
pip install python_speech-features
```

示例

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.io import wavfile 
from features import mfcc, logfbank

# 读取输入文件
sampling_freq, audio = wavfile.read("input_freq.wav")

# 提取MFCC和过滤器组特征
mfcc_features = mfcc(audio, sampling_freq)
filterbank_features = logfbank(audio, sampling_freq)

print('MFCC:\nNumber of windows =', mfcc_features.shape[0])
print('Length of each feature =', mfcc_features.shape[1])
print('\nFilter bank:\nNumber of windows =', filterbank_features.shape[0])
print('Length of each feature =', filterbank_features.shape[1])

# 画出特征图
mfcc_features = mfcc_features.T
plt.matshow(mfcc_features)
plt.title('MFCC')
# 将滤波器组特征可视化，转换矩阵，使时域是水平的
filterbank_features = filterbank_features.T
plt.matshow(filterbank_features)
plt.title('Filter bank')

plt.show()
```

## 创建语音识别器

- 隐马尔可夫模型

**隐马尔可夫模型**(Hidden Markov Models, HMMs)非常擅长建立时间序列数据模型。因为一个音频信号同时也是一个时间序列信号，因此隐马尔可夫模型适用于音频信号的吃醋里。假定输出是通过隐藏状态生成，目标是找到这些隐藏状态，以便对信号建模。

```shell
pip install hmmlearn
```

- 语音文件数据库

数据库文件保存在https://code.google.com/archive/p/hmm-speech-recognition/downloads中。其中包含7个不同的单词，并且每个单词都有15个音频文件与之相关。这是一个较小的数据集。但是足够我们理解如何创建一个语音识别器并识别7个不同的单词。需要为每一类构建一个隐马尔可夫模型。如果想识别新的输入文件中的单词，需要对该文件运行所有的模型，并找出最佳分数的结果。

```python
import os
import argparse 

import numpy as np
from scipy.io import wavfile 
from hmmlearn import hmm
from features import mfcc

# 解析命令行输入的参数
def build_arg_parser():
    parser = argparse.ArgumentParser(description='Trains the HMM classifier')
    parser.add_argument("--input-folder", dest="input_folder", required=True,
            help="Input folder containing the audio files in subfolders")
    return parser

# 创建类处理HMM相关过程
class HMMTrainer(object):
    # 初始化该类
    def __init__(self, model_name='GaussianHMM', n_components=4, cov_type='diag', n_iter=1000):
        self.model_name = model_name  # 模型名字
        self.n_components = n_components  # 隐藏状态的个数
        self.cov_type = cov_type  # 转移矩阵的协方差类型
        self.n_iter = n_iter  # 训练的迭代次数
        self.models = []

        if self.model_name == 'GaussianHMM':
            self.model = hmm.GaussianHMM(n_components=self.n_components, 
                    covariance_type=self.cov_type, n_iter=self.n_iter)
        else:
            raise TypeError('Invalid model type')

    # X是二维数组，其中每一行是13维
    def train(self, X):
        np.seterr(all='ignore')
        self.models.append(self.model.fit(X))

    # 对输入数据运行模型，提取分数
    def get_score(self, input_data):
        return self.model.score(input_data)

if __name__=='__main__':
    # 解析输入参数
    args = build_arg_parser().parse_args()
    input_folder = args.input_folder

    # 初始化HMM模型的变量
    hmm_models = []

    # 解析输入路径
    for dirname in os.listdir(input_folder):
        # 获取子文件夹的名字
        subfolder = os.path.join(input_folder, dirname)

        if not os.path.isdir(subfolder): 
            continue

        # 提取标记
        label = subfolder[subfolder.rfind('/') + 1:]

        # 初始化变量
        X = np.array([])
        y_words = []

        # 迭代所有音频文件(分别保留一个做测试)
        for filename in [x for x in os.listdir(subfolder) if x.endswith('.wav')][:-1]:
            # 读取音频文件
            filepath = os.path.join(subfolder, filename)
            sampling_freq, audio = wavfile.read(filepath)
            
            # 提取MFCC特征
            mfcc_features = mfcc(audio, sampling_freq)

            # 将MFCC特征添加得到X变量
            if len(X) == 0:
                X = mfcc_features
            else:
                X = np.append(X, mfcc_features, axis=0)
            
            # 添加标记
            y_words.append(label)

        print('X.shape =', X.shape)
        
        # 训练并保存HMM模型
        hmm_trainer = HMMTrainer()
        hmm_trainer.train(X)
        hmm_models.append((hmm_trainer, label))
        hmm_trainer = None

    # 测试文件
    input_files = [
            'data/pineapple/pineapple15.wav',
            'data/orange/orange15.wav',
            'data/apple/apple15.wav',
            'data/kiwi/kiwi15.wav'
            ]

    # 为输入数据分类
    for input_file in input_files:
        # 读取音频文件
        sampling_freq, audio = wavfile.read(input_file)

        # 提取MFCC特征
        mfcc_features = mfcc(audio, sampling_freq)

        # 定义变量
        max_score = None
        output_label = None

        # 迭代所有HMM模型，并痛殴每个模型运行输入文件，选取得分最高的模型
        for item in hmm_models:
            hmm_model, label = item
            score = hmm_model.get_score(mfcc_features)
            if score > max_score:
                max_score = score
                output_label = label

        # 对比真实值和预测值
        print("True:", input_file[input_file.find('/')+1:input_file.rfind('/')])
        print("Predicted:", output_label) 
```

